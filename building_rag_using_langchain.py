# -*- coding: utf-8 -*-
"""Building RAG_using_LANGchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lcgxOXOa2ivku5Y3qXe8Ni0C98OuQODX
"""

!pip install -q langchain langchain-community sentence-transformers chromadb transformers torch accelerate bitsandbytes 2>&1 | grep -v "dependency"

pip install langchain==0.1.16 langchain-community==0.0.38 langchain-core==0.1.52

from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import HuggingFacePipeline

from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

print("Initializing Retrieval-Augmented Generation (RAG) Pipeline Demo")
print("=" * 60)

documents = [
    Document(
        page_content="LangChain is a framework for developing applications powered by large language models. It provides tools for document loading, prompt management, chains, and agents.",
        metadata={"source": "doc1"}
    ),
    Document(
        page_content="RAG stands for Retrieval-Augmented Generation. It combines retrieval of relevant documents with text generation to produce more accurate and grounded responses.",
        metadata={"source": "doc2"}
    ),
    Document(
        page_content="Vector databases store embeddings and allow semantic similarity search. They convert text into numerical vectors that capture meaning.",
        metadata={"source": "doc3"}
    ),
    Document(
        page_content="ChromaDB is an open-source vector database designed for embedding-based search. It is lightweight and integrates well with LangChain.",
        metadata={"source": "doc4"}
    ),
    Document(
        page_content="Hugging Face provides thousands of pre-trained models that can be used for NLP, computer vision, and multimodal tasks.",
        metadata={"source": "doc5"}
    ),
    Document(
        page_content="Embeddings are numerical representations of text that capture semantic meaning. Similar texts have similar embeddings in vector space.",
        metadata={"source": "doc6"}
    )
]

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits=text_splitter.split_documents(documents)
print(f"Number of splits: {len(splits)}")

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
print("model loaded successfully")

vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, collection_name="rag_demo")
print("vectorstore created successfully")

retriever = vectorstore.as_retriever(
    search_kwargs={"k": 2}
)

print("retriever created successfully")

print("Using FLANT-5 large model")
model_name = "google/flan-t5-large"
tokenizer=AutoTokenizer.from_pretrained(model_name)

pipe=pipeline(
    "text2text-generation",
    model=model_name,
    tokenizer=tokenizer,
    max_new_tokens=256,
    device = 0 if torch.cuda.is_available() else -1,
    model_kwargs={"do_sample" :True,
    "temperature" :0.7,
    "top_p" : 0.9,}
)
llm = HuggingFacePipeline(pipeline=pipe)
print("llm loaded successfully")

template="""Use the following pieces of context to answer the questions at the end. If you don't know the answer, just say you don't know, don't try to make up the answer. Keep the answer concise and relevant.
{context}
Question: {question}
Helpful Answer:"""

prompt=PromptTemplate(template=template, input_variables=["context","question"])

from langchain.chains import RetrievalQA

print("Building RAG chain....")

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
      retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
)

print("RAG chain built successfully")

print("\n" + "=" * 60)
print("Complete RAG demonstration")
print("="*60)
def ask_question(query):
    print(f"\nüîç Question: {query}")

    # Run RAG chain
    result = qa_chain.invoke({"query": query})

    # -----------------------------
    # Step 1: Show retrieved docs
    # -----------------------------
    print(f"\nüìÑ Retrieved {len(result['source_documents'])} documents:\n")

    for i, doc in enumerate(result["source_documents"], 1):
        print(f"Document {i}:")
        print(f"  Content: {doc.page_content[:150]}...")
        print(f"  Source : {doc.metadata['source']}")
        print("-" * 50)

    # -----------------------------
    # Step 2: Show generated answer
    # -----------------------------
    print("\nü§ñ Step 2: Generating answer using LLM...")
    print(f"\nAnswer:\n{result['result']}")
    print("\n" + "=" * 60)

    print("\nüöÄ Running Demo Queries...\n")

ask_question("What is RAG?")
ask_question("What are embeddings and why are they useful?")
ask_question("Which database should I use for vector storage?")